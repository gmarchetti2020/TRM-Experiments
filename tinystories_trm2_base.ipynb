{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3080ef34",
      "metadata": {
        "id": "3080ef34"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Tiny Recursive Model (TRM) for Text Generation\n",
        "\n",
        "Based on: \"Less is More: Recursive Reasoning with Tiny Networks\" by Jolicoeur-Martineau (2025)\n",
        "\n",
        "This implementation adapts TRM for autoregressive text generation:\n",
        "- Uses recursive reasoning with a tiny 2-layer transformer\n",
        "- Deep supervision with latent state carried across improvement steps\n",
        "- Single network architecture (no hierarchical split)\n",
        "- EMA for training stability\n",
        "\n",
        "Key insight from the paper: smaller networks with deep recursion can outperform\n",
        "larger networks by avoiding overfitting while achieving high effective depth.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import math\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261ce43a",
      "metadata": {
        "id": "261ce43a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from transformers import GPT2Tokenizer\n",
        "import math\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Model Architecture\n",
        "# ============================================================================\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization\"\"\"\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
        "        return x / rms * self.weight\n",
        "\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    \"\"\"Rotary Position Embedding (RoPE)\"\"\"\n",
        "    def __init__(self, dim, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self._build_cache(max_seq_len)\n",
        "\n",
        "    def _build_cache(self, seq_len):\n",
        "        t = torch.arange(seq_len, device=self.inv_freq.device)\n",
        "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n",
        "        emb = torch.cat([freqs, freqs], dim=-1)\n",
        "        self.register_buffer('cos_cached', emb.cos())\n",
        "        self.register_buffer('sin_cached', emb.sin())\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        return self.cos_cached[:seq_len], self.sin_cached[:seq_len]\n",
        "\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat([-x2, x1], dim=-1)\n",
        "\n",
        "\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    # Original cos/sin shape: [seq_len, head_dim]\n",
        "    # q/k shape: [batch_size, n_heads, seq_len, head_dim]\n",
        "    # We need cos/sin to be [1, 1, seq_len, head_dim] for proper broadcasting\n",
        "    cos = cos.unsqueeze(0).unsqueeze(1)  # Corrected from unsqueeze(2)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(1)  # Corrected from unsqueeze(2)\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU activation function\"\"\"\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"Multi-head causal self-attention with RoPE\"\"\"\n",
        "    def __init__(self, dim, n_heads, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        assert dim % n_heads == 0\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = dim // n_heads\n",
        "\n",
        "        self.qkv = nn.Linear(dim, 3 * dim, bias=False)\n",
        "        self.proj = nn.Linear(dim, dim, bias=False)\n",
        "        self.rope = RotaryEmbedding(self.head_dim, max_seq_len)\n",
        "\n",
        "        # Causal mask\n",
        "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.split(C, dim=-1)\n",
        "\n",
        "        q = q.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        cos, sin = self.rope(x)\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
        "        att = att.masked_fill(self.mask[:T, :T], float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "\n",
        "        y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        return self.proj(y)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"Single transformer block with pre-norm\"\"\"\n",
        "    def __init__(self, dim, n_heads, mlp_ratio=4, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        self.norm1 = RMSNorm(dim)\n",
        "        self.attn = CausalSelfAttention(dim, n_heads, max_seq_len)\n",
        "        self.norm2 = RMSNorm(dim)\n",
        "        self.mlp = SwiGLU(dim, dim * mlp_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b106d343",
      "metadata": {
        "id": "b106d343"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TinyRecursiveNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    The core tiny network used in TRM.\n",
        "    Only 2 layers as per the paper's finding that smaller is better.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, n_heads=8, n_layers=2, mlp_ratio=4, max_seq_len=512):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(dim, n_heads, mlp_ratio, max_seq_len)\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "        self.norm = RMSNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class TinyRecursiveModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Tiny Recursive Model for Text Generation\n",
        "\n",
        "    Architecture based on TRM paper:\n",
        "    - Single tiny 2-layer network\n",
        "    - Recursive reasoning with latent z and prediction y\n",
        "    - Deep supervision across multiple improvement steps\n",
        "\n",
        "    For text generation:\n",
        "    - x: embedded input sequence (context)\n",
        "    - y: current token predictions (embedded)\n",
        "    - z: latent reasoning state\n",
        "\n",
        "    The model recursively improves its latent z, then updates y.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        dim=256,\n",
        "        n_heads=8,\n",
        "        n_layers=2,\n",
        "        mlp_ratio=4,\n",
        "        max_seq_len=256,\n",
        "        n_latent_recursions=6,  # n in the paper\n",
        "        n_improvement_cycles=3,  # T in the paper\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.n_latent_recursions = n_latent_recursions\n",
        "        self.n_improvement_cycles = n_improvement_cycles\n",
        "\n",
        "        # Embeddings\n",
        "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
        "        self.pos_emb = nn.Embedding(max_seq_len, dim)\n",
        "\n",
        "        # Single tiny network (key insight: one network is better than two)\n",
        "        self.net = TinyRecursiveNetwork(dim, n_heads, n_layers, mlp_ratio, max_seq_len)\n",
        "\n",
        "        # Projection layers for combining x, y, z\n",
        "        self.combine_xyz = nn.Linear(dim * 3, dim, bias=False)\n",
        "        self.combine_yz = nn.Linear(dim * 2, dim, bias=False)\n",
        "\n",
        "        # Output head\n",
        "        self.output_head = nn.Linear(dim, vocab_size, bias=False)\n",
        "\n",
        "        # Halting head for ACT (simplified - no Q-learning)\n",
        "        self.halt_head = nn.Linear(dim, 1, bias=False)\n",
        "\n",
        "        # Learnable initial states for y and z\n",
        "        self.y_init = nn.Parameter(torch.randn(1, 1, dim) * 0.02)\n",
        "        self.z_init = nn.Parameter(torch.randn(1, 1, dim) * 0.02)\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def get_embeddings(self, input_ids):\n",
        "        \"\"\"Get token + position embeddings\"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        # Clamp input_ids to valid range\n",
        "        input_ids = input_ids.clamp(0, self.vocab_size - 1)\n",
        "        # Clamp position to max_seq_len\n",
        "        T = min(T, self.max_seq_len)\n",
        "        pos = torch.arange(T, device=input_ids.device).unsqueeze(0)\n",
        "        return self.token_emb(input_ids[:, :T]) + self.pos_emb(pos)\n",
        "\n",
        "    def latent_recursion(self, x, y, z):\n",
        "        \"\"\"\n",
        "        Single recursion cycle:\n",
        "        1. Update z n times given (x, y, z)\n",
        "        2. Update y once given (y, z)\n",
        "        \"\"\"\n",
        "        # Latent reasoning: update z n times\n",
        "        for _ in range(self.n_latent_recursions):\n",
        "            combined = self.combine_xyz(torch.cat([x, y, z], dim=-1))\n",
        "            z = self.net(combined)\n",
        "\n",
        "        # Refine prediction: update y given (y, z)\n",
        "        combined_yz = self.combine_yz(torch.cat([y, z], dim=-1))\n",
        "        y = self.net(combined_yz)\n",
        "\n",
        "        return y, z\n",
        "\n",
        "    def deep_recursion(self, x, y, z, use_grad=True):\n",
        "        \"\"\"\n",
        "        Deep recursion with T improvement cycles.\n",
        "        First T-1 cycles without gradients, last cycle with gradients.\n",
        "        \"\"\"\n",
        "        if not use_grad:\n",
        "            # All cycles without gradients (inference)\n",
        "            with torch.no_grad():\n",
        "                for _ in range(self.n_improvement_cycles):\n",
        "                    y, z = self.latent_recursion(x, y, z)\n",
        "            return y.detach(), z.detach()\n",
        "\n",
        "        # T-1 cycles without gradients\n",
        "        with torch.no_grad():\n",
        "            for _ in range(self.n_improvement_cycles - 1):\n",
        "                y, z = self.latent_recursion(x, y, z)\n",
        "\n",
        "        # Last cycle with gradients\n",
        "        y, z = self.latent_recursion(x, y, z)\n",
        "\n",
        "        return y.detach(), z.detach(), self.output_head(y), self.halt_head(y.mean(dim=1))\n",
        "\n",
        "    def forward(self, input_ids, targets=None, n_supervision_steps=4):\n",
        "        \"\"\"\n",
        "        Forward pass with deep supervision.\n",
        "\n",
        "        Args:\n",
        "            input_ids: [B, T] input token IDs\n",
        "            targets: [B, T] target token IDs (for training)\n",
        "            n_supervision_steps: number of deep supervision steps\n",
        "\n",
        "        Returns:\n",
        "            If training: loss\n",
        "            If inference: logits\n",
        "        \"\"\"\n",
        "        B, T = input_ids.shape\n",
        "        T = min(T, self.max_seq_len)\n",
        "        input_ids = input_ids[:, :T]\n",
        "\n",
        "        x = self.get_embeddings(input_ids)\n",
        "\n",
        "        # Initialize y and z\n",
        "        y = self.y_init.expand(B, T, -1).clone()\n",
        "        z = self.z_init.expand(B, T, -1).clone()\n",
        "\n",
        "        if targets is None:\n",
        "            # Inference: just run deep recursion\n",
        "            y, z = self.deep_recursion(x, y, z, use_grad=False)\n",
        "            return self.output_head(y)\n",
        "\n",
        "        # Ensure targets match input length\n",
        "        targets = targets[:, :T]\n",
        "\n",
        "        # Training with deep supervision\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for step in range(n_supervision_steps):\n",
        "            y, z, logits, halt_logit = self.deep_recursion(x, y, z, use_grad=True)\n",
        "\n",
        "            # Cross-entropy loss for token prediction\n",
        "            ce_loss = F.cross_entropy(\n",
        "                logits.view(-1, self.vocab_size),\n",
        "                targets.reshape(-1),\n",
        "                ignore_index=-100\n",
        "            )\n",
        "\n",
        "            # Halting loss (simplified ACT)\n",
        "            with torch.no_grad():\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                mask = (targets != -100)\n",
        "                correct = ((preds == targets) & mask).float().sum() / mask.float().sum().clamp(min=1)\n",
        "            halt_loss = F.binary_cross_entropy_with_logits(\n",
        "                halt_logit.squeeze(-1),\n",
        "                correct.expand(B)\n",
        "            )\n",
        "\n",
        "            total_loss = total_loss + ce_loss + 0.1 * halt_loss\n",
        "\n",
        "        return total_loss / n_supervision_steps\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=50, temperature=0.8, top_k=40):\n",
        "        \"\"\"Generate text autoregressively\"\"\"\n",
        "        self.eval()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop to max_seq_len - 1 to leave room for prediction\n",
        "            idx_cond = input_ids[:, -(self.max_seq_len - 1):]\n",
        "\n",
        "            # Clamp input ids to valid vocab range\n",
        "            idx_cond = idx_cond.clamp(0, self.vocab_size - 1)\n",
        "\n",
        "            # Get predictions\n",
        "            logits = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "\n",
        "        return input_ids\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4ec8b52",
      "metadata": {
        "id": "c4ec8b52"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Dataset\n",
        "# ============================================================================\n",
        "\n",
        "class TinyStoriesDataset(Dataset):\n",
        "    \"\"\"Dataset for TinyStories\"\"\"\n",
        "    def __init__(self, tokenizer, split='train', max_length=256, max_samples=None):\n",
        "        print(f\"Loading TinyStories {split} split...\")\n",
        "        dataset = load_dataset('roneneldan/TinyStories', split=split)\n",
        "\n",
        "        if max_samples:\n",
        "            dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.texts = dataset['text']\n",
        "        self.vocab_size = tokenizer.vocab_size\n",
        "        self.pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
        "        print(f\"Loaded {len(self.texts)} samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        # Add BOS/EOS handling\n",
        "        tokens = self.tokenizer.encode(text, truncation=True, max_length=self.max_length)\n",
        "\n",
        "        # Ensure all tokens are within valid range\n",
        "        tokens = [min(max(t, 0), self.vocab_size - 1) for t in tokens]\n",
        "\n",
        "        # Pad if necessary\n",
        "        if len(tokens) < self.max_length:\n",
        "            tokens = tokens + [self.pad_token_id] * (self.max_length - len(tokens))\n",
        "        else:\n",
        "            tokens = tokens[:self.max_length]\n",
        "\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "        # Input is tokens[:-1], target is tokens[1:]\n",
        "        input_ids = tokens[:-1].clone()\n",
        "        targets = tokens[1:].clone()\n",
        "\n",
        "        # Mask padding in targets (set to -100 to ignore in loss)\n",
        "        targets[targets == self.pad_token_id] = -100\n",
        "\n",
        "        return input_ids, targets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b4ebf80",
      "metadata": {
        "id": "0b4ebf80"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# ============================================================================\n",
        "# Training\n",
        "# ============================================================================\n",
        "\n",
        "class EMA:\n",
        "    \"\"\"Exponential Moving Average for model weights\"\"\"\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.model = model\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = param.data.clone()\n",
        "\n",
        "    def update(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.shadow[name] = (\n",
        "                    self.decay * self.shadow[name] +\n",
        "                    (1 - self.decay) * param.data\n",
        "                )\n",
        "\n",
        "    def apply_shadow(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                self.backup[name] = param.data.clone()\n",
        "                param.data = self.shadow[name]\n",
        "\n",
        "    def restore(self):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data = self.backup[name]\n",
        "\n",
        "\n",
        "def train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    epochs=5,\n",
        "    lr=1e-4,\n",
        "    warmup_steps=1000,\n",
        "    n_supervision_steps=4,\n",
        "    ema_decay=0.999,\n",
        "    save_path='trm_tinystories.pt'\n",
        "):\n",
        "    \"\"\"Training loop with deep supervision and EMA\"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "    ema = EMA(model, decay=ema_decay)\n",
        "\n",
        "    # Learning rate scheduler with warmup\n",
        "    def lr_schedule(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps\n",
        "        return 1.0\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)\n",
        "\n",
        "    global_step = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "\n",
        "        for input_ids, targets in pbar:\n",
        "            input_ids = input_ids.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = model(input_ids, targets, n_supervision_steps=n_supervision_steps)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            ema.update()\n",
        "\n",
        "            global_step += 1\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.6f}'})\n",
        "\n",
        "        # Validation\n",
        "        ema.apply_shadow()\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for input_ids, targets in tqdm(val_loader, desc='Validation'):\n",
        "                input_ids = input_ids.to(device)\n",
        "                targets = targets.to(device)\n",
        "                loss = model(input_ids, targets, n_supervision_steps=n_supervision_steps)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f'Epoch {epoch+1} - Val Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Generate sample\n",
        "        prompt = \"Once upon a time\"\n",
        "        prompt_ids = torch.tensor([tokenizer.encode(prompt)], device=device)\n",
        "        generated = model.generate(prompt_ids, max_new_tokens=100)\n",
        "        generated_text = tokenizer.decode(generated[0].tolist())\n",
        "        print(f'Generated: {generated_text[:300]}...\\n')\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'ema_shadow': ema.shadow,\n",
        "                'epoch': epoch,\n",
        "                'val_loss': val_loss\n",
        "            }, save_path)\n",
        "            print(f'Saved best model with val_loss={val_loss:.4f}')\n",
        "\n",
        "        ema.restore()\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27e19300",
      "metadata": {
        "id": "27e19300"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# Main\n",
        "# ============================================================================\n",
        "\n",
        "#def main():\n",
        "# Configuration\n",
        "config = {\n",
        "    'vocab_size': 50257,  # GPT-2 vocab\n",
        "    'dim': 256,           # Hidden dimension\n",
        "    'n_heads': 8,         # Attention heads\n",
        "    'n_layers': 2,        # Only 2 layers (key insight from paper)\n",
        "    'mlp_ratio': 4,\n",
        "    'max_seq_len': 128,   # Reduced for stability\n",
        "    'n_latent_recursions': 4,  # n in paper (reduced for memory)\n",
        "    'n_improvement_cycles': 2,  # T in paper (reduced for memory)\n",
        "\n",
        "    # Training\n",
        "    'batch_size': 256,     # 16 Reduced batch size\n",
        "    'epochs': 3,\n",
        "    'lr': 1e-4,\n",
        "    'warmup_steps': 500,\n",
        "    'n_supervision_steps': 3,  # Deep supervision steps during training\n",
        "    'max_train_samples': 2000000,  # Limit for faster training demo\n",
        "    'max_val_samples': 20000,\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b99e6c0c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b99e6c0c",
        "outputId": "a74dbccc-d5b7-46b6-cd32-2831a9a9074d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model parameters: 28,191,232 (28.19M)\n",
            "Effective depth per supervision step: 20\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Model\n",
        "model = TinyRecursiveModel(\n",
        "    vocab_size=config['vocab_size'],\n",
        "    dim=config['dim'],\n",
        "    n_heads=config['n_heads'],\n",
        "    n_layers=config['n_layers'],\n",
        "    mlp_ratio=config['mlp_ratio'],\n",
        "    max_seq_len=config['max_seq_len'],\n",
        "    n_latent_recursions=config['n_latent_recursions'],\n",
        "    n_improvement_cycles=config['n_improvement_cycles'],\n",
        ")\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'Model parameters: {n_params:,} ({n_params/1e6:.2f}M)')\n",
        "print(f'Effective depth per supervision step: {config[\"n_improvement_cycles\"] * (config[\"n_latent_recursions\"] + 1) * config[\"n_layers\"]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d245663e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d245663e",
        "outputId": "6286e6a8-8e45-4e72-ea96-ce2e6ea7f84f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyStories train split...\n",
            "Loaded 2000000 samples\n",
            "Loading TinyStories validation split...\n",
            "Loaded 20000 samples\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Datasets\n",
        "train_dataset = TinyStoriesDataset(\n",
        "    tokenizer,\n",
        "    split='train',\n",
        "    max_length=config['max_seq_len'] + 1,  # +1 for next token prediction\n",
        "    max_samples=config['max_train_samples']\n",
        ")\n",
        "val_dataset = TinyStoriesDataset(\n",
        "    tokenizer,\n",
        "    split='validation',\n",
        "    max_length=config['max_seq_len'] + 1,\n",
        "    max_samples=config['max_val_samples']\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config['batch_size'],\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22bd8b48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22bd8b48",
        "outputId": "89ca0b96-deee-445b-9f70-1085a58c55d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|██████████| 7813/7813 [1:18:09<00:00,  1.67it/s, loss=2.0730, lr=0.000100]\n",
            "Validation: 100%|██████████| 79/79 [00:22<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Val Loss: 2.0817\n",
            "Generated: Once upon a time, there was a boy named Timmy. Timmy loved to play with his toys all day long. One day, Timmy's mom gave him a big, colorful toy car. Timmy was so happy to have it, but his mom warned him not to eat it because it might not fit in the toy car.\n",
            "\n",
            "Timmy didn't like any idea, so he asked ...\n",
            "\n",
            "Saved best model with val_loss=2.0817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|██████████| 7813/7813 [1:18:09<00:00,  1.67it/s, loss=1.8105, lr=0.000100]\n",
            "Validation: 100%|██████████| 79/79 [00:22<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Val Loss: 1.7954\n",
            "Generated: Once upon a time, there was a little girl named Lily. One day, she went to visit her grandma's house. Lily loved the color pink and her grandma wore them. \n",
            "\n",
            "As they walked around grandma's house, Lily noticed that her grandma's grandma was wearing a sparkly dress. Mom asked Lily where her grandma wa...\n",
            "\n",
            "Saved best model with val_loss=1.7954\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|██████████| 7813/7813 [1:18:09<00:00,  1.67it/s, loss=1.7581, lr=0.000100]\n",
            "Validation: 100%|██████████| 79/79 [00:22<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Val Loss: 1.6934\n",
            "Generated: Once upon a time, there was a little girl named Lily. She loved to play outside and pick flowers in the garden. One day, she found a tiny seed and watered it with a big smile. She watered it every day and waited for it to grow.\n",
            "\n",
            "One day, Lily went outside to play. She saw a butterfly and said hello ...\n",
            "\n",
            "Saved best model with val_loss=1.6934\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Train\n",
        "model = train(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    epochs=config['epochs'],\n",
        "    lr=config['lr'],\n",
        "    warmup_steps=config['warmup_steps'],\n",
        "    n_supervision_steps=config['n_supervision_steps'],\n",
        ")\n",
        "\n",
        "print('\\nTraining complete!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e1a2b6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77e1a2b6",
        "outputId": "e3564c88-3d8c-458c-f560-e28d6fdf9160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Generated Stories ===\n",
            "\n",
            "Prompt: \"Once upon a time\"\n",
            "Story: Once upon a time, there was a big lion. He was very lazy, but he always went for a walk. One day, he saw a little girl walking by. She was running around in a bush and looking at the lion. The lion didn't like that, so he kept trying to catch her. \n",
            "\n",
            "As he walked, he heard a voice. It was a friendly voice calling for help! He looked around and noticed a little girl walking on the path. The girl was smiling and she waved her hand. The lion was happy to help her and hopped over to her. \n",
            "\n",
            "The lion led her through the forest together, until it was out of sight. The little girl was so excited to see the lion and wanted to get closer\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: \"The little girl\"\n",
            "Story: The little girl was very excited. She had a tray for her birthday party. It was big and blue and had lots of things inside. She couldn't wait to play with it! \n",
            "\n",
            "The little girl spent the day with the tray. But then she started to get tired. She had to go to the toilet, so she was very tired. She was so tired. \n",
            "\n",
            "Just then, a kind old man came by. He had a big bag of food. He saw the little girl was so happy. He said to the little girl, \"Let's go back to my house today.\" The little girl was so happy she could take a nap.\n",
            "\n",
            "The old man smiled and said, \"It's okay. This is your\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: \"One day, a rabbit\"\n",
            "Story: One day, a rabbit and a cat were in the woods. They wanted to find a new place to play. They looked and looked, but they could not find it.\n",
            "\n",
            "The rabbit said, \"I think we should be friends. We can share the things we have.\"\n",
            "\n",
            "The cat agreed. They said, \"Let's play! We can find a nice place with food and flowers!\"\n",
            "\n",
            "The rabbit said, \"Yes, let's go!\"\n",
            "\n",
            "They found a big tree and decided to be friends. They walked and walked until they found a big tree. They climbed up and put the tree there. They were happy, and they looked around for a place to live.\n",
            "\n",
            "The rabbit, the bird, and the bear were the\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: \"Tom and his friend\"\n",
            "Story: Tom and his friend Sam are friends. They like to play in the park. They have a big slide and a swing. But they have a problem. They are stubborn. They do not have a ladder.\n",
            "\n",
            "One day, they see a big bird on the slide. It has a long neck and sharp teeth. Tom and Sam want to see it. They think it is a toy bird. They want to see it closer.\n",
            "\n",
            "They climb on the slide and wait for the bird. They are not careful. They wait. They are careful. They wait for the bird to come fast.\n",
            "\n",
            "The bird sees them. It is scared. It flies away. It flew away. It was not hurt. Soon, it was near the bird.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Final generation examples\n",
        "model.eval()\n",
        "ema = EMA(model)\n",
        "\n",
        "prompts = [\n",
        "    \"Once upon a time\",\n",
        "    \"The little girl\",\n",
        "    \"One day, a rabbit\",\n",
        "    \"Tom and his friend\"\n",
        "]\n",
        "\n",
        "print('\\n=== Generated Stories ===\\n')\n",
        "for prompt in prompts:\n",
        "    prompt_ids = torch.tensor([tokenizer.encode(prompt)], device=device)\n",
        "    generated = model.generate(prompt_ids, max_new_tokens=150, temperature=0.8)\n",
        "    text = tokenizer.decode(generated[0].tolist())\n",
        "    print(f'Prompt: \"{prompt}\"')\n",
        "    print(f'Story: {text}\\n')\n",
        "    print('-' * 50 + '\\n')\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "trm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "H100 80GB",
      "name": "tinystories_trm2_base.ipynb"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}